{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64022da9",
   "metadata": {},
   "source": [
    "# Subset scenes, DEMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6bbc771c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import shapely\n",
    "import geopandas as gpd\n",
    "import rasterio as rio\n",
    "from rasterio.windows import Window\n",
    "import rasterio.mask \n",
    "import shutil\n",
    "import glob\n",
    "from osgeo import gdal\n",
    "import xarray as xr\n",
    "from PIL import Image\n",
    "import time\n",
    "from scipy import ndimage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97fad99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set paths\n",
    "loc = 'frontrange'\n",
    "track = 'des'\n",
    "\n",
    "desktop = '/Users/qbren/Desktop'\n",
    "main_dir = f'{desktop}/taco/projects/atmospheric_correction/proc/training_data'\n",
    "igram_dir = f'{main_dir}/{track}_crop_{loc}_noise_era5'\n",
    "subsets_dir = f'subsets_{loc}_{track}'\n",
    "veloc_fn = f'{main_dir}/veloc_crop/{track}_veloc_{loc}.tif'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccf8dbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clump_size(array, quantile_threshold=0.99, return_mask=False):\n",
    "    quantile = np.quantile(np.absolute(array), q=quantile_threshold) # find threshold based on quantile\n",
    "    mask = np.where(np.absolute(array)>quantile, 1, 0) # get areas above threshold\n",
    "    s = [[1,1,1],[1,1,1],[1,1,1]] # structure for clumping kernel \n",
    "    features, count = ndimage.label(mask, s) # clump features\n",
    "    feature_ids, feature_counts = np.unique(features, return_counts=True) # count pixels in features\n",
    "    feature_counts = np.delete(feature_counts, feature_counts.argmax()) # throw away background\n",
    "    \n",
    "    if return_mask==True:\n",
    "        return mask\n",
    "    \n",
    "    return feature_counts.mean()\n",
    "    \n",
    "# I think the appropriate mean for keeping a subset is 3 pixels per clump with a quantile threshold of 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce74038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on:  S1AA_20180626T130936_20180708T130937_VVP012_INT40_G_ueF_E4CA\n",
      "total subsets: 113, total counter: 935\n",
      "working on:  S1AA_20180708T130937_20180720T130938_VVP012_INT40_G_ueF_866A\n",
      "total subsets: 116, total counter: 948\n",
      "working on:  S1AA_20180720T130938_20180801T130938_VVP012_INT40_G_ueF_2D55\n",
      "total subsets: 119, total counter: 954\n",
      "working on:  S1AA_20180801T130938_20180813T130944_VVP012_INT40_G_ueF_DF60\n",
      "total subsets: 129, total counter: 952\n",
      "working on:  S1AA_20180813T130944_20180825T130944_VVP012_INT40_G_ueF_F926\n",
      "total subsets: 125, total counter: 952\n",
      "working on:  S1AA_20180825T130944_20180906T130945_VVP012_INT40_G_ueF_1556\n",
      "total subsets: 90, total counter: 958\n",
      "working on:  S1AA_20180906T130945_20180918T130945_VVP012_INT40_G_ueF_F15C\n",
      "total subsets: 48, total counter: 976\n",
      "working on:  S1AA_20180918T130945_20180930T130946_VVP012_INT40_G_ueF_3CC2\n",
      "total subsets: 123, total counter: 952\n",
      "working on:  S1AA_20180930T130946_20181012T130946_VVP012_INT40_G_ueF_99C9\n",
      "total subsets: 0, total counter: 961\n",
      "working on:  S1AA_20190621T130946_20190703T130947_VVP012_INT40_G_ueF_77BD\n",
      "total subsets: 46, total counter: 973\n",
      "working on:  S1AA_20190703T130947_20190715T130948_VVP012_INT40_G_ueF_7B98\n",
      "total subsets: 100, total counter: 959\n",
      "working on:  S1AA_20190715T130948_20190727T130949_VVP012_INT40_G_ueF_DEC2\n",
      "total subsets: 101, total counter: 964\n",
      "working on:  S1AA_20190727T130949_20190808T130942_VVP012_INT40_G_ueF_C7F5\n",
      "total subsets: 113, total counter: 907\n",
      "working on:  S1AA_20190808T130942_20190820T130943_VVP012_INT40_G_ueF_4D64\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# load, crop, and save rasters\n",
    "\n",
    "# list interferograms\n",
    "dirs = os.listdir(igram_dir)\n",
    "subsets_total = 0\n",
    "subsets_desired = 1500\n",
    "igram_loop = 0\n",
    "\n",
    "while subsets_total < subsets_desired: # loop until number of desired subsets is satisfied\n",
    "    igram_loop += 1\n",
    "\n",
    "    # loop through interferograms\n",
    "    for i, idir in enumerate(dirs):\n",
    "        if i > 0:\n",
    "            subsets_total += subsets\n",
    "            print(f'total subsets: {subsets}, total counter: {counter}')\n",
    "        cwd = f'{igram_dir}/{idir}'\n",
    "        os.chdir(cwd)\n",
    "\n",
    "        # select the line of sight displacement\n",
    "        allfiles = os.listdir(cwd)\n",
    "        for fn in allfiles:\n",
    "            if fn[-12:] == 'los_disp.tif':\n",
    "                noise_tif = fn\n",
    "            elif fn[-7:] == 'dem.tif':\n",
    "                dem_tif = fn\n",
    "            elif fn[-8:] == 'ERA5.tif':\n",
    "                era5_tif = fn\n",
    "        noise_tif_fn = f'{igram_dir}/{idir}/{noise_tif}'\n",
    "        dem_tif_fn =  f'{igram_dir}/{idir}/{dem_tif}'\n",
    "        era5_tif_fn = f'{igram_dir}/{idir}/{era5_tif}'\n",
    "        xsize, ysize = 125, 125  # define the test image size\n",
    "        print(\"working on: \", idir)\n",
    "\n",
    "        subsets = 0 # reset subset number\n",
    "        counter = 0 # reset counter \n",
    "        timeout = time.time() + 60*3 # set time to spend on each igram\n",
    "\n",
    "        # loop until subsetting is finished\n",
    "        while time.time() < timeout:\n",
    "            counter += 1\n",
    "\n",
    "            # open src image\n",
    "            noise_src = rio.open(noise_tif_fn)\n",
    "            noise_rs = noise_src.read(1) # also open as np array\n",
    "\n",
    "            dem_src = rio.open(dem_tif_fn)\n",
    "            dem_rs = dem_src.read(1) # also open as np array\n",
    "\n",
    "            veloc_src = rio.open(veloc_fn)\n",
    "            veloc_rs = veloc_src.read(1) # also open as np array\n",
    "            \n",
    "            era5_src = rio.open(era5_tif_fn)\n",
    "            era5_rs = era5_src.read(1)\n",
    "\n",
    "            # create bounds for new origin location\n",
    "            xmin, xmax = 0, noise_src.width - xsize\n",
    "            ymin, ymax = 0, noise_src.height - ysize\n",
    "            xoff, yoff = np.random.randint(xmin, xmax), np.random.randint(ymin, ymax)\n",
    "\n",
    "            temp_noise = noise_rs[yoff:(yoff+ysize), xoff:(xoff+xsize)]\n",
    "            temp_dem = dem_rs[yoff:(yoff+ysize), xoff:(xoff+xsize)]\n",
    "            temp_veloc = veloc_rs[yoff:(yoff+ysize), xoff:(xoff+xsize)]\n",
    "            temp_era5 = era5_rs[yoff:(yoff+ysize), xoff:(xoff+xsize)]\n",
    "\n",
    "            if not 0.0 in temp_noise: # only save if there are no nodata values in subset\n",
    "                if np.median(temp_dem) >= 3300: # only save if subset is at high elevation\n",
    "                    if clump_size(temp_veloc) >= 3.0: # only save subsets with big moving features\n",
    "                        window = Window(xoff, yoff, xsize, ysize)\n",
    "                        noise_transform = noise_src.window_transform(window)\n",
    "                        dem_transform = dem_src.window_transform(window)\n",
    "                        veloc_transform = veloc_src.window_transform(window)\n",
    "                        era5_transform = era5_src.window_transform(window)\n",
    "\n",
    "                        # write subset\n",
    "                        noise_profile = noise_src.profile\n",
    "                        dem_profile = dem_src.profile\n",
    "                        veloc_profile = veloc_src.profile\n",
    "                        era5_profile = era5_src.profile\n",
    "\n",
    "                        noise_profile.update({\n",
    "                            'height': xsize,\n",
    "                            'width': ysize,\n",
    "                            'transform': noise_transform})\n",
    "                        dem_profile.update({\n",
    "                            'height': xsize,\n",
    "                            'width': ysize,\n",
    "                            'transform': dem_transform})\n",
    "                        veloc_profile.update({\n",
    "                            'height': xsize,\n",
    "                            'width': ysize,\n",
    "                            'transform': veloc_transform})\n",
    "                        era5_profile.update({\n",
    "                            'height': xsize,\n",
    "                            'width': ysize,\n",
    "                            'transform': era5_transform})\n",
    "\n",
    "                        noise_out_fn = f'{desktop}/{subsets_dir}/int/{track}{igram_loop}.{subsets}_{noise_tif[0:-13]}.tif'\n",
    "                        dem_out_fn = f'{desktop}/{subsets_dir}/dem/{track}{igram_loop}.{subsets}_{noise_tif[0:-13]}.tif'\n",
    "                        veloc_out_fn = f'{desktop}/{subsets_dir}/veloc/{track}{igram_loop}.{subsets}_{noise_tif[0:-13]}.tif'\n",
    "                        era5_out_fn = f'{desktop}/{subsets_dir}/era5/{track}{igram_loop}.{subsets}_{noise_tif[0:-13]}.tif'\n",
    "\n",
    "                        with rasterio.open(noise_out_fn, 'w', **noise_profile) as noise_dst:\n",
    "                            # Read the data from the window and write it to the output raster\n",
    "                            noise_dst.write(noise_src.read(window=window))\n",
    "                        noise_src.close()\n",
    "\n",
    "                        with rasterio.open(dem_out_fn, 'w', **dem_profile) as dem_dst:\n",
    "                            # Read the data from the window and write it to the output raster\n",
    "                            dem_dst.write(dem_src.read(window=window))\n",
    "                        dem_src.close()\n",
    "\n",
    "                        with rasterio.open(veloc_out_fn, 'w', **veloc_profile) as veloc_dst:\n",
    "                            # Read the data from the window and write it to the output raster\n",
    "                            veloc_dst.write(veloc_src.read(window=window))\n",
    "                        veloc_src.close()\n",
    "                        \n",
    "                        with rasterio.open(era5_out_fn, 'w', **era5_profile) as era5_dst:\n",
    "                            # Read the data from the window and write it to the output raster\n",
    "                            era5_dst.write(era5_src.read(window=window))\n",
    "                        era5_src.close()\n",
    "\n",
    "                        subsets+=1 # update index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5398e4",
   "metadata": {},
   "source": [
    "## Interpolate gaps if need be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90dc7803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list subsets \n",
    "noise_fns = os.listdir(f'{desktop}/{subsets_dir}/int')\n",
    "\n",
    "def list_tifs(my_fns):\n",
    "    my_list = []\n",
    "    for i in my_fns:\n",
    "        if i[-4:] == '.tif':\n",
    "            my_list.append(i)\n",
    "    return my_list\n",
    "\n",
    "noise_list = list_tifs(noise_fns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df215d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "zeros=[]\n",
    "\n",
    "for i, fn in enumerate(noise_list):\n",
    "        noise_fn = f'{desktop}/{subsets_dir}/int/{fn}'\n",
    "\n",
    "        noise_src = rio.open(noise_fn)\n",
    "        noise = noise_src.read(1) # also open as np array\n",
    "        zeros.append(np.count_nonzero(noise==0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8db89ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(zeros, bins=300);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f4fd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpolate holes\n",
    "#for i, fn in enumerate(noise_list):\n",
    "#        noise_fn = f'{desktop}/{subsets_dir}/int/{fn}'\n",
    "#\n",
    "#        noise_src = rio.open(noise_fn)\n",
    "#        noise = noise_src.read(1) # also open as np array\n",
    "#        noise = xr.DataArray(noise).interpolate_na(dim='dim_0', method='linear').fillna(value=0).values\n",
    "#\n",
    "#        out_path = f'{desktop}/{subsets_dir}/int_interp/{fn}'\n",
    "\n",
    "#        im = Image.fromarray(noise)\n",
    "#        im.save(out_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
